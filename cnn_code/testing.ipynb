{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout, MaxPooling1D, Reshape, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_data(class_name):\n",
    "    global class_counts_test\n",
    "    \n",
    "    path_test = os.path.join(test_path, class_name)\n",
    "    class_units_test = np.fromfile(path_test, dtype=int, count=-1, sep=' ', offset=0)\n",
    "    units_test = np.split(class_units_test, class_units_test.size // 1080, 0) # 1080 = 3 sec * 360Hz\n",
    "    class_counts_test[class_name] = class_units_test.size // 1080\n",
    "    \n",
    "    return units_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'C:\\\\cygwin\\\\home\\\\User\\\\1studing\\\\wfdb\\\\GettingData\\\\cnn_code'\n",
    "test_path = os.path.join(base_path, 'data_files_test')\n",
    "model_save_path = os.path.join(base_path, 'model\\\\cnn_model.h5')\n",
    "model_weights_path = os.path.join(base_path, 'model\\\\cnn_model_weights.hdf5')\n",
    "\n",
    "classes = ['N', 'LBBB', 'RBBB', 'B', 'T', 'VT', 'SVTA', 'AFIB', 'AFL', 'NOD', 'IVR', 'V']\n",
    "class_mapping = {'N': 0, 'LBBB': 1, 'RBBB': 2, 'B': 3, 'T': 4, 'VT': 5, 'SVTA': 6, 'AFIB': 7, 'AFL': 8, 'NOD': 9, 'IVR': 10, 'V': 11}\n",
    "class_counts_test = {'N': 0, 'LBBB': 0, 'RBBB': 0, 'B': 0, 'T': 0, 'VT': 0, 'SVTA': 0, 'AFIB': 0, 'AFL': 0, 'NOD': 0, 'IVR': 0, 'V': 0}\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for class_i in classes:\n",
    "    class_units_test = get_class_data(class_i)\n",
    "    Y_test  += [class_mapping[class_i]] * class_counts_test[class_i]\n",
    "    X_test += class_units_test\n",
    "\n",
    "#print('X_test')\n",
    "#print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalize(dataset):\n",
    "\n",
    "    mu = np.mean(dataset)\n",
    "    sigma = np.std(dataset)\n",
    "    \n",
    "    return (dataset - mu) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7926/1 - 78s - loss: 2.8125 - accuracy: 0.7338\n",
      "\n",
      "Accuracy on test data: 0.73\n",
      "\n",
      "Loss on test data: 0.88\n"
     ]
    }
   ],
   "source": [
    "XX_test = np.asarray(X_test)\n",
    "YY_test = np.asarray(Y_test)\n",
    "XX_test = XX_test.astype(\"float32\")\n",
    "YY_test = YY_test.astype(\"float32\")\n",
    "\n",
    "XXX_test = feature_normalize(XX_test)\n",
    "\n",
    "num_classes = 12\n",
    "YY_test = to_categorical(YY_test, num_classes)\n",
    "\n",
    "model_test = load_model(model_save_path)\n",
    "model_test.load_weights(model_weights_path) \n",
    "\n",
    "BATCH_SIZE = 400\n",
    "\n",
    "score = model_test.evaluate(XXX_test, YY_test, batch_size = BATCH_SIZE, verbose=2)\n",
    "\n",
    "print(\"\\nAccuracy on test data: %0.2f\" % score[1])\n",
    "print(\"\\nLoss on test data: %0.2f\" % score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicting on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(validations, predictions):\n",
    "    \n",
    "    #print(metrics.confusion_matrix(validations, predictions, LABELS))\n",
    "    #print()\n",
    "\n",
    "    matrix = metrics.confusion_matrix(validations, predictions)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    sns.heatmap(matrix,\n",
    "                cmap=\"coolwarm\",\n",
    "                linecolor='white',\n",
    "                linewidths=1,\n",
    "                xticklabels=LABELS,\n",
    "                yticklabels=LABELS,\n",
    "                annot=True,\n",
    "                fmt=\"d\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Confusion matrix for test data ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Confusion matrix for test data ---\\n\")\n",
    "\n",
    "LABELS = [\"N\", \"LBBB\", \"RBBB\", \"B\", \"T\", \"VT\", \"SVTA\", \"AFIB\", \"AFL\", \"NOD\", \"IVR\", \"V\"]\n",
    "\n",
    "Y_pred_test = model_test.predict(XXX_test)\n",
    "\n",
    "max_Y_pred_test = np.argmax(Y_pred_test, axis=1)\n",
    "max_Y_test = np.argmax(YY_test, axis=1)\n",
    "\n",
    "show_confusion_matrix(max_Y_test, max_Y_pred_test)\n",
    "\n",
    "#print(\"\\n--- Classification report for test data ---\\n\")\n",
    "#print(classification_report(max_Y_test, max_Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Confusion matrix for test data ---\\n\")\n",
    "\n",
    "temp_rand = np.arange(7926)\n",
    "#np.random.shuffle(temp_rand)\n",
    "#temp_part_rand = temp_rand[:10]\n",
    "temp_part_rand = temp_rand[np.array([0,1000,2000,3000,4000,5000,6000,7000,2500,7920])]\n",
    "\n",
    "XXX_part_test = XXX_test[temp_part_rand, :1080]\n",
    "Y_part_test = YY_test[temp_part_rand]\n",
    "\n",
    "Y_part_pred_test = model_test.predict(XXX_part_test)\n",
    "\n",
    "max_Y_part_pred_test = np.argmax(Y_part_pred_test, axis=1)\n",
    "max_Y_part_test = np.argmax(Y_part_test, axis=1)\n",
    "\n",
    "print(Y_part_test)\n",
    "print('--------------')\n",
    "print(max_Y_part_test)\n",
    "print('--------------')\n",
    "print(max_Y_part_pred_test)\n",
    "\n",
    "show_confusion_matrix(max_Y_part_test, max_Y_part_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Partial predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicting on external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'C:\\\\cygwin\\\\home\\\\User\\\\1studing\\\\wfdb\\\\GettingData\\\\cnn_code'\n",
    "predict_path = os.path.join(base_path, 'data_files_predict')\n",
    "\n",
    "class_name = '214'\n",
    "path_predict = os.path.join(predict_path, class_name)\n",
    "X_pred = np.fromfile(path_predict, dtype=int, count=-1, sep=' ', offset=0)\n",
    "\n",
    "temp_predict = np.split(X_pred, X_pred.size // 1080, 0) \n",
    "\n",
    "XX_pred = np.asarray(temp_predict)\n",
    "XX_pred = XX_pred.astype(\"float32\")\n",
    "\n",
    "\n",
    "X_predict = feature_normalize(XX_pred)\n",
    "\n",
    "#print(X_predict)\n",
    "print('--------------------------------------')\n",
    "\n",
    "Y_predict = model_test.predict_proba(X_predict)\n",
    "\n",
    "commom_len = len(Y_predict) \n",
    "#print(len(Y_predict))\n",
    "\n",
    "#print(Y_predict)\n",
    "\n",
    "\n",
    "#print(\"\\n--- Confusion matrix for test data ---\\n\")\n",
    "\n",
    "LABELS = [\"N\", \"LBBB\", \"RBBB\", \"B\", \"T\", \"VT\", \"SVTA\", \"AFIB\", \"AFL\", \"NOD\", \"IVR\", \"V\"]\n",
    "\n",
    "Y_pred_test = model_test.predict(X_predict)\n",
    "\n",
    "max_Y_pred_test = np.argmax(Y_pred_test, axis=1)\n",
    "\n",
    "print('max')\n",
    "print(max_Y_pred_test)\n",
    "\n",
    "class_mapping_predict = {0: 'N', 1: 'LBBB', 2: 'RBBB', 3: 'B', 4: 'T', 5: 'VT', 6: 'SVTA', 7:'AFIB', 8: 'AFL', 9: 'NOD', 10: 'IVR', 11: 'V'}\n",
    "class_counts_predict = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0}\n",
    "\n",
    "for class_i in max_Y_pred_test:\n",
    "    class_counts_predict[class_i] += 1\n",
    "\n",
    "print('Probabilities')\n",
    "for class_i in class_counts_predict:\n",
    "    print(class_mapping_predict[class_i] + ' ' + str(class_counts_predict[class_i]) + ' ' + str(class_counts_predict[class_i] // commom_len * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
